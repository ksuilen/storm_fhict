{"url_to_unified_index": {"https://nexla.com/enterprise-ai/low-rank-adaptation-of-large-language-models/": 12, "https://incubity.ambilio.com/lora-vs-fine-tuning-optimizing-llm-adaptation/": 15, "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971": 23, "https://github.com/huggingface/peft/issues/622": 25, "https://www.labellerr.com/blog/comprehensive-guide-for-fine-tuning-of-llms/": 20, "https://www.ionio.ai/blog/enhancing-model-performance-fine-tuning-lora-qlora": 24, "https://machinelearningmastery.com/custom-fine-tuning-for-domain-specific-llms/": 19, "https://arxiv.org/pdf/2505.07329": 22, "https://medium.com/@manindersingh120996/practical-guide-to-fine-tune-llms-with-lora-c835a99d7593": 21, "https://arxiv.org/abs/2010.12864": 4, "https://www.datacamp.com/tutorial/fine-tuning-large-language-models": 3, "https://www.geeksforgeeks.org/fine-tuning-large-language-models-llms-using-qlora/": 1, "https://arxiv.org/abs/2404.08699": 8, "https://www.geeksforgeeks.org/what-is-low-rank-adaptation-lora/": 5, "https://medium.com/ubiai-nlp/fine-tuning-llm-a-deep-dive-into-advanced-techniques-for-optimal-model-performance-289affdfaf61": 9, "https://www.sciencedirect.com/science/article/pii/S2949719125000202": 6, "https://finlora-docs.readthedocs.io/en/latest/fine-tuning/lora_methods.html": 7, "https://www.turing.com/resources/finetuning-large-language-models": 2, "https://mljourney.com/fine-tuning-llm-using-lora/": 11, "https://link.springer.com/article/10.1007/s11704-024-40663-9": 16, "https://agathon.ai/insights/demystifying-lora": 18, "https://medium.com/data-science-at-microsoft/understanding-and-implementing-lora-theory-and-practical-code-for-efficient-fine-tuning-cffda1e9ff97": 14, "https://hahminlew.github.io/finetuning/lora/": 13, "https://zilliz.com/learn/lora-explained-low-rank-adaptation-for-fine-tuning-llms": 10, "https://medium.com/@benuehlinger/low-rank-adaptation-lora-for-fine-tuning-llms-2a04ba28b3a2": 17, "https://arxiv.org/abs/2410.21228": 27, "https://dev.to/ankush_mahore/mastering-llm-hyperparameter-tuning-for-optimal-performance-1gc1": 29, "https://llmmodels.org/blog/10-hyperparameter-tuning-tips-for-llm-fine-tuning/": 30, "https://medium.com/@mgenoj/hyper-parameter-fine-tuning-in-large-language-models-a-deep-dive-into-lora-and-qlora-1e5ece85c3b5": 28, "https://arxiv.org/pdf/2501.00365": 26, "https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html": 32, "https://www.evnekquest.com/post/lora-revolutionizing-fine-tuning-for-large-language-models-with-efficiency-and-scalability": 33, "https://medium.com/@rohitbohra23051994/fine-tuning-large-language-models-llms-with-lora-and-quantization-a-step-by-step-guide-24369b5447ce": 34, "https://www.ibm.com/think/topics/lora": 31, "https://medium.com/@rachittayal7/my-experiences-with-finetuning-llms-using-lora-b9c90f1839c6": 35, "https://www.irjes.com/Papers/vol13-issue2/J13027782.pdf": 37, "https://scisimple.com/en/articles/2025-05-13-the-impact-of-fine-tuning-on-language-models--a9ngqdo": 36, "https://dl.acm.org/doi/10.1145/3597307": 38, "https://www.tonic.ai/guides/ethical-fine-tuning-llm-synthetic-data": 41, "https://code-b.dev/blog/lora-finetuning-for-llms": 47, "https://machinelearningmastery.com/5-problems-encountered-fine-tuning-llms-with-solutions/": 46, "https://www.linkedin.com/pulse/fine-tuning-llms-using-lora-basics-rahul-pandey-aicne": 39, "https://www.researchgate.net/publication/387517749_Machine_Learning_Ethics_and_Responsible_AI_Balancing_Safety_and_Trust_in_Fine-Tuning": 40, "https://arxiv.org/html/2411.10915v1": 42, "https://aclanthology.org/2021.naacl-main.296.pdf": 43, "https://www.sciencedirect.com/science/article/pii/S0363811124000869": 44, "https://arxiv.org/html/2401.09410v2": 45}, "url_to_info": {"https://nexla.com/enterprise-ai/low-rank-adaptation-of-large-language-models/": {"url": "https://nexla.com/enterprise-ai/low-rank-adaptation-of-large-language-models/", "description": "Low-rank adaptation, or LoRA, is an advanced fine-tuning technique designed to reduce the number of trainable parameters in large language models without significantly compromising performance. By decomposing weight updates into low-rank matrices, LoRA enables LLMs to adapt to specific tasks while minimizing computational requirements.", "snippets": ["Low-rank adaptation, or LoRA, is an advanced fine-tuning technique designed to reduce the number of trainable parameters in large language models without significantly compromising performance. By decomposing weight updates into low-rank matrices, LoRA enables LLMs to adapt to specific tasks while minimizing computational requirements."], "title": "Low-rank Adaptation of Large Language Models\u2014Implementation Guide - Nexla", "meta": {"query": "LowRank Adaptation LoRA principles in LLMs"}, "citation_uuid": -1}, "https://incubity.ambilio.com/lora-vs-fine-tuning-optimizing-llm-adaptation/": {"url": "https://incubity.ambilio.com/lora-vs-fine-tuning-optimizing-llm-adaptation/", "description": "4. Impact on Model Performance. LoRA's implementation maintains a delicate balance between efficiency and performance. Despite reducing the number of trainable parameters, LoRA consistently demonstrates competitive, if not superior, performance compared to traditional full fine-tuning approaches.", "snippets": ["4. Impact on Model Performance. LoRA's implementation maintains a delicate balance between efficiency and performance. Despite reducing the number of trainable parameters, LoRA consistently demonstrates competitive, if not superior, performance compared to traditional full fine-tuning approaches."], "title": "LoRA vs. Fine-Tuning: Optimizing LLM Adaptation - Incubity by Ambiio", "meta": {"query": "impact of LoRA on model performance versus traditional finetuning"}, "citation_uuid": -1}, "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971": {"url": "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971", "description": "Try different learning rates (e.g., 1e-5, 3e-5, 5e-5) and monitor the model's performance to find the optimal rate that converges quickly and effectively.", "snippets": ["Try different learning rates (e.g., 1e-5, 3e-5, 5e-5) and monitor the model's performance to find the optimal rate that converges quickly and effectively."], "title": "Tuning parameters to train LLMs (Large Language Models)", "meta": {"query": "how to determine optimal learning rate for finetuning LLMs"}, "citation_uuid": -1}, "https://github.com/huggingface/peft/issues/622": {"url": "https://github.com/huggingface/peft/issues/622", "description": "Specifically, the paper reports that fine-tuning using LoRA generally results in performance at par with or better than full fine-tuning of the model, however, throughout our experiments I observe a performance lower than full fine-tuning by an absolute margin of ~4-6% in terms of RougeL score. Sharing some of the training details below:", "snippets": ["Specifically, the paper reports that fine-tuning using LoRA generally results in performance at par with or better than full fine-tuning of the model, however, throughout our experiments I observe a performance lower than full fine-tuning by an absolute margin of ~4-6% in terms of RougeL score. Sharing some of the training details below:"], "title": "LoRA results in 4-6% lower performance compared to full fine-tuning", "meta": {"query": "model performance issues with LoRA finetuning"}, "citation_uuid": -1}, "https://www.labellerr.com/blog/comprehensive-guide-for-fine-tuning-of-llms/": {"url": "https://www.labellerr.com/blog/comprehensive-guide-for-fine-tuning-of-llms/", "description": "Case Studies for Fine-tuning of LLMs In the below section, we discuss some case studies where fine-tuning models (LLMs) have come in handy to solve real-world problems. Enhancing Legal Document Analysis through LLM Fine-Tuning Legal documents, characterized by intricate language and specialized terminology, pose a substantial obstacle.", "snippets": ["Case Studies for Fine-tuning of LLMs In the below section, we discuss some case studies where fine-tuning models (LLMs) have come in handy to solve real-world problems. Enhancing Legal Document Analysis through LLM Fine-Tuning Legal documents, characterized by intricate language and specialized terminology, pose a substantial obstacle."], "title": "Retraining LLM: A Comprehensive Guide - Labellerr", "meta": {"query": "How does LLM finetuning work?"}, "citation_uuid": -1}, "https://www.ionio.ai/blog/enhancing-model-performance-fine-tuning-lora-qlora": {"url": "https://www.ionio.ai/blog/enhancing-model-performance-fine-tuning-lora-qlora", "description": "Adjusting these parameters, along with the dropout rate and lora_alpha, allows for fine-tuning the model based on performance and resource considerations, finding the optimal setup for the task at hand. Setting up the training parameters. Define training arguments and create a Trainer instance. A note on training: To perform fine-tuning, the", "snippets": ["Adjusting these parameters, along with the dropout rate and lora_alpha, allows for fine-tuning the model based on performance and resource considerations, finding the optimal setup for the task at hand. Setting up the training parameters. Define training arguments and create a Trainer instance. A note on training: To perform fine-tuning, the", "PEFT Fine-tuning, or Parameter Efficient Fine-tuning, is a set of techniques designed to make model training more efficient. This fine-tuned adapter is then loaded into the pre-trained model for use during inference. When using PEFT to train a model with LoRA or QLoRA, the hyperparameters of the low-rank adaptation process can be defined in a LoRA config. Adjusting these parameters, along with the dropout rate and lora_alpha, allows for fine-tuning the model based on performance and resource considerations, finding the optimal setup for the task at hand. Optimize Adapter Usage: When using adapters, understand that the size of the LoRA adapter obtained through fine-tuning is typically small compared to the pre-trained base model."], "title": "Enhancing Model Performance: The Impact of Fine-tuning with LoRA & QLoRA", "meta": {"query": "model performance issues with LoRA finetuning"}, "citation_uuid": -1}, "https://machinelearningmastery.com/custom-fine-tuning-for-domain-specific-llms/": {"url": "https://machinelearningmastery.com/custom-fine-tuning-for-domain-specific-llms/", "description": "Custom fine-tuning with domain-specific datasets helps your model better understand specialized terminology and domain-related requirements and nuances. When working with Hugging Face models, fine-tuning typically requires identifying the type of LLM and target task it was pre-trained on (for instance, text generation), and loading the appropriate auto class for managing that type of model (in this example, AutoModelForCausalLM). This process involves instantiating a TrainingArguments and Training instances, where we set configuration aspects like the number of training rounds and learning rate, calling the train() method, and saving the fine-tuned model. lora_config = LoraConfig( r=8, lora_alpha=16, target_modules=[\"query_key_value\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\" ) model = get_peft_model(model, lora_config) training_args = TrainingArguments( output_dir=\"./results\", per_device_train_batch_size=2, num_train_epochs=2, logging_steps=10, save_steps=20, save_total_limit=2, optim=\"paged_adamw_8bit\", learning_rate=2e-4, ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset, ) trainer.train() model.save_pretrained(\"./custom-finetuned-llm\") tokenizer.save_pretrained(\"./custom-finetuned-llm\")", "snippets": ["Custom fine-tuning with domain-specific datasets helps your model better understand specialized terminology and domain-related requirements and nuances. When working with Hugging Face models, fine-tuning typically requires identifying the type of LLM and target task it was pre-trained on (for instance, text generation), and loading the appropriate auto class for managing that type of model (in this example, AutoModelForCausalLM). This process involves instantiating a TrainingArguments and Training instances, where we set configuration aspects like the number of training rounds and learning rate, calling the train() method, and saving the fine-tuned model. lora_config = LoraConfig( r=8, lora_alpha=16, target_modules=[\"query_key_value\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\" ) model = get_peft_model(model, lora_config) training_args = TrainingArguments( output_dir=\"./results\", per_device_train_batch_size=2, num_train_epochs=2, logging_steps=10, save_steps=20, save_total_limit=2, optim=\"paged_adamw_8bit\", learning_rate=2e-4, ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset, ) trainer.train() model.save_pretrained(\"./custom-finetuned-llm\") tokenizer.save_pretrained(\"./custom-finetuned-llm\")"], "title": "Custom Fine-Tuning for Domain-Specific LLMs", "meta": {"query": "domains using LoRA for LLM finetuning"}, "citation_uuid": -1}, "https://arxiv.org/pdf/2505.07329": {"url": "https://arxiv.org/pdf/2505.07329", "description": "To address this critical need for secure and accessible private fine-tuning, we present a protocol where a client (data owner) interactively orchestrates LoRA fine-tuning of an open-source LLM, securely outsourcing the most demanding computations to a server (or a network of worker nodes).", "snippets": ["To address this critical need for secure and accessible private fine-tuning, we present a protocol where a client (data owner) interactively orchestrates LoRA fine-tuning of an open-source LLM, securely outsourcing the most demanding computations to a server (or a network of worker nodes)."], "title": "Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption", "meta": {"query": "LLM finetuning with LoRA ethical considerations"}, "citation_uuid": -1}, "https://medium.com/@manindersingh120996/practical-guide-to-fine-tune-llms-with-lora-c835a99d7593": {"url": "https://medium.com/@manindersingh120996/practical-guide-to-fine-tune-llms-with-lora-c835a99d7593", "description": "LoRA(PeFT) makes Fine-Tuning Open-Source LLMs like LLama,Mistral,Gemma,etc on a domain specific task quite easy on consumer grade Hardware\u2026", "snippets": ["LoRA(PeFT) makes Fine-Tuning Open-Source LLMs like LLama,Mistral,Gemma,etc on a domain specific task quite easy on consumer grade Hardware\u2026"], "title": "Practical Guide to Fine-tune LLMs with LoRA - Medium", "meta": {"query": "LoRA finetuning LLM model transparency accountability"}, "citation_uuid": -1}, "https://arxiv.org/abs/2010.12864": {"url": "https://arxiv.org/abs/2010.12864", "description": "Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task", "snippets": ["Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task"], "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning", "meta": {"query": "impact of finetuning on biases in language models"}, "citation_uuid": -1}, "https://www.datacamp.com/tutorial/fine-tuning-large-language-models": {"url": "https://www.datacamp.com/tutorial/fine-tuning-large-language-models", "description": "A Step-by-Step Guide to Fine-tuning a LLM. Run and edit the code from this tutorial online. Run code. We already know that Fine-tuning is the process of taking a pre-trained model and updating its parameters by training on a dataset specific to your task. So, let's exemplify this concept by fine-tuning a real model.", "snippets": ["Learn how fine-tuning large language models (LLMs) improves their performance in tasks like language translation, sentiment analysis, and text generation."], "title": "Fine-Tuning LLMs: A Guide With Examples - DataCamp", "meta": {"query": "What is LLM finetuning?"}, "citation_uuid": -1}, "https://www.geeksforgeeks.org/fine-tuning-large-language-models-llms-using-qlora/": {"url": "https://www.geeksforgeeks.org/fine-tuning-large-language-models-llms-using-qlora/", "description": "Fine-Tuning Large Language Models (LLMs) Using QLoRA | GeeksforGeeks Fine-Tuning Large Language Models (LLMs) Using QLoRA Fine-tuning large language models (LLMs) is used for adapting LLM's to specific tasks, improving their accuracy and making them more efficient. QLoRA is a advanced fine-tuning method that quantizes LLMs to reduce memory usage and applies Low-Rank Adaptation (LoRA) to train a subset of model parameters. Fine Tuning Large Language Model (LLM) Large Language Models (LLMs) have dramatically transformed natural language processing (NLP), excelling in tasks like text generation, translation, summarization, and question-answering. Instruction Tuning for Large Language Models Instruction tuning refers to the process of fine-tuning a pre-trained language model on a dataset composed of instructions and corresponding outputs.", "snippets": ["Fine-Tuning Large Language Models (LLMs) Using QLoRA | GeeksforGeeks Fine-Tuning Large Language Models (LLMs) Using QLoRA Fine-tuning large language models (LLMs) is used for adapting LLM's to specific tasks, improving their accuracy and making them more efficient. QLoRA is a advanced fine-tuning method that quantizes LLMs to reduce memory usage and applies Low-Rank Adaptation (LoRA) to train a subset of model parameters. Fine Tuning Large Language Model (LLM) Large Language Models (LLMs) have dramatically transformed natural language processing (NLP), excelling in tasks like text generation, translation, summarization, and question-answering. Instruction Tuning for Large Language Models Instruction tuning refers to the process of fine-tuning a pre-trained language model on a dataset composed of instructions and corresponding outputs."], "title": "Fine-Tuning Large Language Models (LLMs) Using QLoRA", "meta": {"query": "LoRA theory in large language model finetuning"}, "citation_uuid": -1}, "https://arxiv.org/abs/2404.08699": {"url": "https://arxiv.org/abs/2404.08699", "description": "In an era where language models are increasingly integrated into decision-making and communication, understanding the biases within Large Language Models (LLMs) becomes imperative, especially when these models are applied in the economic and political domains. This work investigates the impact of fine-tuning and data selection on economic and political biases in LLMs. In this context, we", "snippets": ["In an era where language models are increasingly integrated into decision-making and communication, understanding the biases within Large Language Models (LLMs) becomes imperative, especially when these models are applied in the economic and political domains. This work investigates the impact of fine-tuning and data selection on economic and political biases in LLMs. In this context, we"], "title": "PoliTune: Analyzing the Impact of Data Selection and Fine-Tuning on ...", "meta": {"query": "impact of finetuning on biases in language models"}, "citation_uuid": -1}, "https://www.geeksforgeeks.org/what-is-low-rank-adaptation-lora/": {"url": "https://www.geeksforgeeks.org/what-is-low-rank-adaptation-lora/", "description": "LoRA addresses the challenge of fine-tuning massive deep learning models by reducing the number of trainable parameters, thus making the process more efficient and scalable. ... Transfer learning is a machine learning technique where a model trained on one task is repurposed as the foundation for a second task. This approach is beneficial when", "snippets": ["LoRA addresses the challenge of fine-tuning massive deep learning models by reducing the number of trainable parameters, thus making the process more efficient and scalable. ... Transfer learning is a machine learning technique where a model trained on one task is repurposed as the foundation for a second task. This approach is beneficial when"], "title": "What is Low Rank Adaptation (LoRA)? - GeeksforGeeks", "meta": {"query": "What is LoRA in machine learning?"}, "citation_uuid": -1}, "https://medium.com/ubiai-nlp/fine-tuning-llm-a-deep-dive-into-advanced-techniques-for-optimal-model-performance-289affdfaf61": {"url": "https://medium.com/ubiai-nlp/fine-tuning-llm-a-deep-dive-into-advanced-techniques-for-optimal-model-performance-289affdfaf61", "description": "Dynamic Learning Rate Adjustment Adjusting the learning rate dynamically during fine-tuning allows the model to learn more effectively, adapting its speed to the complexity of new tasks.", "snippets": ["Dynamic Learning Rate Adjustment Adjusting the learning rate dynamically during fine-tuning allows the model to learn more effectively, adapting its speed to the complexity of new tasks."], "title": "Fine-Tuning LLM: A Deep dive into advanced techniques for ... - Medium", "meta": {"query": "LLM finetuning techniques learning rate adjustment"}, "citation_uuid": -1}, "https://www.sciencedirect.com/science/article/pii/S2949719125000202": {"url": "https://www.sciencedirect.com/science/article/pii/S2949719125000202", "description": "To solve these issues, the idea of only training a separate new module called adapter during the fine-tuning process, was introduced, named Parameter-Efficient Transfer Learning for NLP (Houlsby et al., 2019). This review takes into account many research papers, including some pivotal ones, for instance, \u201cParameter-Efficient Fine-Tuning Methods for Pre-trained Language Models\u201d by Xu et al. This led to the development of methods that trained only specific parts of the model (Zaken et al., 2021) during the fine-tuning process and methods such as LoRA (Hu et al., 2021) which introduced the idea of having separate modules called adapters to be trained during the fine-tuning process.", "snippets": ["To solve these issues, the idea of only training a separate new module called adapter during the fine-tuning process, was introduced, named Parameter-Efficient Transfer Learning for NLP (Houlsby et al., 2019). This review takes into account many research papers, including some pivotal ones, for instance, \u201cParameter-Efficient Fine-Tuning Methods for Pre-trained Language Models\u201d by Xu et al. This led to the development of methods that trained only specific parts of the model (Zaken et al., 2021) during the fine-tuning process and methods such as LoRA (Hu et al., 2021) which introduced the idea of having separate modules called adapters to be trained during the fine-tuning process."], "title": "The fine art of fine-tuning: A structured review of advanced LLM fine ...", "meta": {"query": "LLM finetuning techniques learning rate adjustment"}, "citation_uuid": -1}, "https://finlora-docs.readthedocs.io/en/latest/fine-tuning/lora_methods.html": {"url": "https://finlora-docs.readthedocs.io/en/latest/fine-tuning/lora_methods.html", "description": "Finetuning. Low-Rank Adaptation Methods for Large Language Models. 1. What is LoRA? 2. Foundations of LoRA. 2.1 Ranks; 2.2 Fine-tuning Strategies. 2.2.1 Fine-tuning Without Adapters; 2.2.2 Fine-tuning With Adapters (Parameter Efficient Fine-Tuning\u2014PEFT) 3 Low-Rank Adaptation (LoRA) 4 Quantized Low-Rank Adaptation (QLoRA) 5 LoRA Methods with", "snippets": ["Finetuning. Low-Rank Adaptation Methods for Large Language Models. 1. What is LoRA? 2. Foundations of LoRA. 2.1 Ranks; 2.2 Fine-tuning Strategies. 2.2.1 Fine-tuning Without Adapters; 2.2.2 Fine-tuning With Adapters (Parameter Efficient Fine-Tuning\u2014PEFT) 3 Low-Rank Adaptation (LoRA) 4 Quantized Low-Rank Adaptation (QLoRA) 5 LoRA Methods with"], "title": "Low-Rank Adaptation Methods for Large Language Models", "meta": {"query": "key theoretical foundations of LowRank Adaptation LoRA for finetuning large language models"}, "citation_uuid": -1}, "https://www.turing.com/resources/finetuning-large-language-models": {"url": "https://www.turing.com/resources/finetuning-large-language-models", "description": "Fine-tuning LLMs bridge this gap by refining pre-trained models with task-specific data, enhancing accuracy while maintaining broad language knowledge. Fine-tuning is a transfer learning technique where a pre-trained model is further trained on new data to adapt it to specific tasks. Proper data preparation is essential for fine-tuning as it directly impacts the model's ability to learn and generalize effectively, ultimately leading to improved performance and accuracy in generating task-specific outputs. Fine-tuning models on specific company data, unique domains, or unique tasks helps with the accurate analysis and understanding of the sentiment expressed in textual content, enabling businesses to gain valuable insights from customer feedback, social media posts, and product reviews.", "snippets": ["Fine-tuning LLMs bridge this gap by refining pre-trained models with task-specific data, enhancing accuracy while maintaining broad language knowledge. Fine-tuning is a transfer learning technique where a pre-trained model is further trained on new data to adapt it to specific tasks. Proper data preparation is essential for fine-tuning as it directly impacts the model's ability to learn and generalize effectively, ultimately leading to improved performance and accuracy in generating task-specific outputs. Fine-tuning models on specific company data, unique domains, or unique tasks helps with the accurate analysis and understanding of the sentiment expressed in textual content, enabling businesses to gain valuable insights from customer feedback, social media posts, and product reviews."], "title": "What is Fine-Tuning LLM? Methods & Step-by-Step Guide in 2025 - Turing", "meta": {"query": "What is LLM finetuning?"}, "citation_uuid": -1}, "https://mljourney.com/fine-tuning-llm-using-lora/": {"url": "https://mljourney.com/fine-tuning-llm-using-lora/", "description": "LoRA (Low-Rank Adaptation) is a fine-tuning technique that modifies only a small subset of a pre-trained model\u2019s weights, thereby reducing the number of trainable parameters. Instead of updating all model weights, LoRA introduces trainable low-rank matrices into the transformer layers, which are then adjusted during fine-tuning. Train the Model: Use frameworks like Hugging Face\u2019s Trainer API to fine-tune only the LoRA layers while keeping the rest of the model weights frozen. 2model.save_pretrained(\"./fine_tuned_llm_lora\") Companies are fine-tuning pre-trained models like GPT-4, LLaMA, and Falcon using LoRA to create domain-specific chatbots for customer support, legal assistance, and healthcare consultations. Fine-tuning LLMs using LoRA provides a cost-effective, efficient, and scalable way to adapt pre-trained models to specific tasks. By leveraging low-rank matrices, LoRA enables parameter-efficient fine-tuning, reducing memory usage and computational demands while maintaining strong performance.", "snippets": ["Low-Rank Adaptation (LoRA) is a technique that significantly reduces the computational overhead while maintaining strong performance. In this article, we will explore fine-tuning LLM using LoRA, its benefits, implementation, and best practices. Whether you're a researcher, engineer, or AI enthusiast, this guide will help you understand how to"], "title": "Fine-Tuning LLM Using LoRA - ML Journey", "meta": {"query": "best practices for implementing LoRA in finetuning large language models"}, "citation_uuid": -1}, "https://link.springer.com/article/10.1007/s11704-024-40663-9": {"url": "https://link.springer.com/article/10.1007/s11704-024-40663-9", "description": "Low-Rank Adaptation (LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It", "snippets": ["Low-Rank Adaptation (LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It"], "title": "A survey on LoRA of large language models - Springer", "meta": {"query": "successful applications of LoRA in large language models"}, "citation_uuid": -1}, "https://agathon.ai/insights/demystifying-lora": {"url": "https://agathon.ai/insights/demystifying-lora", "description": "Furthermore, ethical implications arise when deploying AI models in sensitive contexts\u2014ensuring responsible innovation is paramount. Organisations must navigate these challenges carefully to avoid unintended consequences. Future Directions and Research Opportunities. The future of LoRA is promising, as ongoing research continues to uncover", "snippets": ["Furthermore, ethical implications arise when deploying AI models in sensitive contexts\u2014ensuring responsible innovation is paramount. Organisations must navigate these challenges carefully to avoid unintended consequences. Future Directions and Research Opportunities. The future of LoRA is promising, as ongoing research continues to uncover"], "title": "Demystifying LoRA - agathon.ai", "meta": {"query": "LoRA finetuning ethical implications in AI"}, "citation_uuid": -1}, "https://medium.com/data-science-at-microsoft/understanding-and-implementing-lora-theory-and-practical-code-for-efficient-fine-tuning-cffda1e9ff97": {"url": "https://medium.com/data-science-at-microsoft/understanding-and-implementing-lora-theory-and-practical-code-for-efficient-fine-tuning-cffda1e9ff97", "description": "The key idea is that instead of updating all of a model\u2019s parameters during fine-tuning, LoRA introduces low-rank trainable matrices to adjust only a small subset of weights. This makes LoRA particularly suited for large models where fine-tuning all parameters is not feasible due to memory and hardware constraints. LoRA resolves this issue by freezing the pre-trained weights of the model and introducing a low-rank trainable approximation to fine-tune the model. When fine-tuning a pre-trained model with LoRA, a trainable low-rank matrix pair (B \u22c5 A) is added to the frozen weight matrix W. In conclusion, LoRA enables the fine-tuning of large models in scenarios where computational resources are limited, making it a powerful tool in the era of massive, pre-trained models.", "snippets": ["The key idea is that instead of updating all of a model\u2019s parameters during fine-tuning, LoRA introduces low-rank trainable matrices to adjust only a small subset of weights. This makes LoRA particularly suited for large models where fine-tuning all parameters is not feasible due to memory and hardware constraints. LoRA resolves this issue by freezing the pre-trained weights of the model and introducing a low-rank trainable approximation to fine-tune the model. When fine-tuning a pre-trained model with LoRA, a trainable low-rank matrix pair (B \u22c5 A) is added to the frozen weight matrix W. In conclusion, LoRA enables the fine-tuning of large models in scenarios where computational resources are limited, making it a powerful tool in the era of massive, pre-trained models."], "title": "Understanding and implementing LoRA: Theory and practical code ... - Medium", "meta": {"query": "LoRA theory in large language model finetuning"}, "citation_uuid": -1}, "https://hahminlew.github.io/finetuning/lora/": {"url": "https://hahminlew.github.io/finetuning/lora/", "description": "A key idea of Low-rank adaptation is to indirectly train rank decomposition matrices instead of directly updating model weights. Problem Statement. As shown in equations, equation (1) shows the the conditional language modeling objective in case of full finetuning, while equation (2) is the LoRA objective.", "snippets": ["A key idea of Low-rank adaptation is to indirectly train rank decomposition matrices instead of directly updating model weights. Problem Statement. As shown in equations, equation (1) shows the the conditional language modeling objective in case of full finetuning, while equation (2) is the LoRA objective."], "title": "[Paper Review] LoRA: Low-Rank Adaptation of Large Language Models", "meta": {"query": "key theoretical foundations of LowRank Adaptation LoRA for finetuning large language models"}, "citation_uuid": -1}, "https://zilliz.com/learn/lora-explained-low-rank-adaptation-for-fine-tuning-llms": {"url": "https://zilliz.com/learn/lora-explained-low-rank-adaptation-for-fine-tuning-llms", "description": "LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning LLMs by introducing low-rank trainable weight matrices into specific model layers.", "snippets": ["LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning LLMs by introducing low-rank trainable weight matrices into specific model layers."], "title": "LoRA Explained: Low-Rank Adaptation for Fine-Tuning LLMs", "meta": {"query": "LowRank Adaptation LoRA principles in LLMs"}, "citation_uuid": -1}, "https://medium.com/@benuehlinger/low-rank-adaptation-lora-for-fine-tuning-llms-2a04ba28b3a2": {"url": "https://medium.com/@benuehlinger/low-rank-adaptation-lora-for-fine-tuning-llms-2a04ba28b3a2", "description": "Edward Hu, the inventor of LoRA, explained the efficiency gain on the scale of fine-tuning of GPT-3. When comparing LoRA to full fine-tuning, trainable parameters decreased from 175B to 4.7M and", "snippets": ["Edward Hu, the inventor of LoRA, explained the efficiency gain on the scale of fine-tuning of GPT-3. When comparing LoRA to full fine-tuning, trainable parameters decreased from 175B to 4.7M and"], "title": "Low Rank Adaptation (LoRA) For Fine-Tuning LLMs | by Ben U - Medium", "meta": {"query": "challenges of using LoRA for finetuning LLMs"}, "citation_uuid": -1}, "https://arxiv.org/abs/2410.21228": {"url": "https://arxiv.org/abs/2410.21228", "description": "View a PDF of the paper titled LoRA vs Full Fine-tuning: An Illusion of Equivalence, by Reece Shuttleworth and 3 other authors Second, we show that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. View a PDF of the paper titled LoRA vs Full Fine-tuning: An Illusion of Equivalence, by Reece Shuttleworth and 3 other authors Bibliographic Explorer Toggle Connected Papers Toggle Litmaps Toggle scite.ai Toggle alphaXiv Toggle Links to Code Toggle DagsHub Toggle GotitPub Toggle Huggingface Toggle Links to Code Toggle ScienceCast Toggle Replicate Toggle Spaces Toggle Spaces Toggle Core recommender toggle IArxiv recommender toggle", "snippets": ["View a PDF of the paper titled LoRA vs Full Fine-tuning: An Illusion of Equivalence, by Reece Shuttleworth and 3 other authors Second, we show that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. View a PDF of the paper titled LoRA vs Full Fine-tuning: An Illusion of Equivalence, by Reece Shuttleworth and 3 other authors Bibliographic Explorer Toggle Connected Papers Toggle Litmaps Toggle scite.ai Toggle alphaXiv Toggle Links to Code Toggle DagsHub Toggle GotitPub Toggle Huggingface Toggle Links to Code Toggle ScienceCast Toggle Replicate Toggle Spaces Toggle Spaces Toggle Core recommender toggle IArxiv recommender toggle"], "title": "Title: LoRA vs Full Fine-tuning: An Illusion of Equivalence - arXiv.org", "meta": {"query": "impact of LoRA on model performance versus traditional finetuning"}, "citation_uuid": -1}, "https://dev.to/ankush_mahore/mastering-llm-hyperparameter-tuning-for-optimal-performance-1gc1": {"url": "https://dev.to/ankush_mahore/mastering-llm-hyperparameter-tuning-for-optimal-performance-1gc1", "description": "Mastering LLM Hyperparameter Tuning for Optimal Performance - DEV Community However, to get the best performance from your model, it\u2019s essential to tune the hyperparameters. This blog will walk you through the basics of hyperparameter tuning for LLMs and provide practical tips to optimize your model. Unlike parameters (which are learned by the model), hyperparameters need to be set manually and can significantly impact performance. Tuning hyperparameters allows you to strike the perfect balance between model accuracy and training time. In grid search, you manually define a set of hyperparameter values and train the model for every combination of these parameters. By understanding and adjusting key hyperparameters like learning rate, batch size, and model architecture, you can significantly improve your model\u2019s results.", "snippets": ["Mastering LLM Hyperparameter Tuning for Optimal Performance - DEV Community However, to get the best performance from your model, it\u2019s essential to tune the hyperparameters. This blog will walk you through the basics of hyperparameter tuning for LLMs and provide practical tips to optimize your model. Unlike parameters (which are learned by the model), hyperparameters need to be set manually and can significantly impact performance. Tuning hyperparameters allows you to strike the perfect balance between model accuracy and training time. In grid search, you manually define a set of hyperparameter values and train the model for every combination of these parameters. By understanding and adjusting key hyperparameters like learning rate, batch size, and model architecture, you can significantly improve your model\u2019s results."], "title": "Mastering LLM Hyperparameter Tuning for Optimal Performance", "meta": {"query": "how to determine optimal learning rate for finetuning LLMs"}, "citation_uuid": -1}, "https://llmmodels.org/blog/10-hyperparameter-tuning-tips-for-llm-fine-tuning/": {"url": "https://llmmodels.org/blog/10-hyperparameter-tuning-tips-for-llm-fine-tuning/", "description": "Hyperparameter tuning is crucial for optimizing Large Language Models (LLMs) during fine-tuning. Here are the top 10 tips to improve your LLM's performance and efficiency: Understand Hyperparameters: Hyperparameters control the training process and affect how well the model learns.Tune model hyperparameters (e.g., sequence length) and training hyperparameters (e.g., batch size) to improve", "snippets": ["Hyperparameter tuning is crucial for optimizing Large Language Models (LLMs) during fine-tuning. Here are the top 10 tips to improve your LLM's performance and efficiency: Understand Hyperparameters: Hyperparameters control the training process and affect how well the model learns.Tune model hyperparameters (e.g., sequence length) and training hyperparameters (e.g., batch size) to improve"], "title": "10 Hyperparameter Tuning Tips for LLM Fine-Tuning", "meta": {"query": "hyperparameter tuning challenges in LoRA finetuning"}, "citation_uuid": -1}, "https://medium.com/@mgenoj/hyper-parameter-fine-tuning-in-large-language-models-a-deep-dive-into-lora-and-qlora-1e5ece85c3b5": {"url": "https://medium.com/@mgenoj/hyper-parameter-fine-tuning-in-large-language-models-a-deep-dive-into-lora-and-qlora-1e5ece85c3b5", "description": "Hyperparameter fine-tuning is a critical step in optimizing the performance of Large Language Models, and techniques like LoRA and QLoRA offer exciting avenues for making this process more efficient.", "snippets": ["Hyperparameter fine-tuning is a critical step in optimizing the performance of Large Language Models, and techniques like LoRA and QLoRA offer exciting avenues for making this process more efficient."], "title": "Hyper-parameter Fine-Tuning in Large Language Models: A Deep ... - Medium", "meta": {"query": "hyperparameter tuning challenges in LoRA finetuning"}, "citation_uuid": -1}, "https://arxiv.org/pdf/2501.00365": {"url": "https://arxiv.org/pdf/2501.00365", "description": "Low-rank Adaptation Fig. 1. LoRA with foundation models in diverse domains. resources required for both training and fine-tuning [20]. Although traditional fine-tuning methods involving full parameters updates have demonstrated effectiveness across various tasks [21], [22], their computational demands often", "snippets": ["Low-rank Adaptation Fig. 1. LoRA with foundation models in diverse domains. resources required for both training and fine-tuning . Although traditional fine-tuning methods involving full parameters updates have demonstrated effectiveness across various tasks , , their computational demands often"], "title": "Low-Rank Adaptation for Foundation Models: A Comprehensive Review", "meta": {"query": "key theoretical foundations of LowRank Adaptation LoRA for finetuning large language models"}, "citation_uuid": -1}, "https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html": {"url": "https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html", "description": "The implementation of LoRA is relatively straight-forward. We can think of it as a modified forward pass for the fully connected layers in an LLM. In pseudo-code, this looks like as follows: ... We will revisit this topic in a more detailed article in the future. But as a takeaway here, LoRA can be used to finetuning an LLM on an instruction", "snippets": ["The implementation of LoRA is relatively straight-forward. We can think of it as a modified forward pass for the fully connected layers in an LLM. In pseudo-code, this looks like as follows: ... We will revisit this topic in a more detailed article in the future. But as a takeaway here, LoRA can be used to finetuning an LLM on an instruction"], "title": "Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)", "meta": {"query": "successful implementation of LoRA in LLM finetuning"}, "citation_uuid": -1}, "https://www.evnekquest.com/post/lora-revolutionizing-fine-tuning-for-large-language-models-with-efficiency-and-scalability": {"url": "https://www.evnekquest.com/post/lora-revolutionizing-fine-tuning-for-large-language-models-with-efficiency-and-scalability", "description": "After diving deep into the concepts of LoRA, it's time to apply the theory to practice. We'll leverage the PEFT (Parameter-Efficient Fine-Tuning) library along with bitsandbytes to fine-tune a Hugging Face language model. This practical implementation showcases how to efficiently train large models with minimal computational resources.", "snippets": ["After diving deep into the concepts of LoRA, it's time to apply the theory to practice. We'll leverage the PEFT (Parameter-Efficient Fine-Tuning) library along with bitsandbytes to fine-tune a Hugging Face language model. This practical implementation showcases how to efficiently train large models with minimal computational resources."], "title": "LoRA: Revolutionizing Fine-Tuning for Large Language Models with ...", "meta": {"query": "LoRA theory in large language model finetuning"}, "citation_uuid": -1}, "https://medium.com/@rohitbohra23051994/fine-tuning-large-language-models-llms-with-lora-and-quantization-a-step-by-step-guide-24369b5447ce": {"url": "https://medium.com/@rohitbohra23051994/fine-tuning-large-language-models-llms-with-lora-and-quantization-a-step-by-step-guide-24369b5447ce", "description": "LoRA for parameter-efficient fine-tuning. Hugging Face's SFTTrainer for simplified training. By combining these techniques, you can fine-tune massive LLMs on modest hardware, making powerful AI", "snippets": ["LoRA for parameter-efficient fine-tuning. Hugging Face's SFTTrainer for simplified training. By combining these techniques, you can fine-tune massive LLMs on modest hardware, making powerful AI"], "title": "Fine-Tuning Large Language Models (LLMs) with LoRA and ... - Medium", "meta": {"query": "challenges of using LoRA for finetuning LLMs"}, "citation_uuid": -1}, "https://www.ibm.com/think/topics/lora": {"url": "https://www.ibm.com/think/topics/lora", "description": "LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. LoRA adds low-rank matrices to the frozen original machine learning model. This significantly reduces the trainable parameters of the model and the GPU memory requirement for the training process, which is another significant challenge when it comes to fine-tuning or training large models. To implement LoRA fine tuning with HuggingFace using Python and PyTorch, developers can use the parameter-efficient fine-tuning (PEFT) library to inject the LoRA adapters into the model and use them as the update matrices. Removing this resiliency can make models less accurate, so the rank of update matrices can be tuned as a part of the LoRA process.", "snippets": ["LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. LoRA adds low-rank matrices to the frozen original machine learning model. This significantly reduces the trainable parameters of the model and the GPU memory requirement for the training process, which is another significant challenge when it comes to fine-tuning or training large models. To implement LoRA fine tuning with HuggingFace using Python and PyTorch, developers can use the parameter-efficient fine-tuning (PEFT) library to inject the LoRA adapters into the model and use them as the update matrices. Removing this resiliency can make models less accurate, so the rank of update matrices can be tuned as a part of the LoRA process."], "title": "What is LoRA (low-rank adaption)? - IBM", "meta": {"query": "LowRank Adaptation LoRA principles in LLMs"}, "citation_uuid": -1}, "https://medium.com/@rachittayal7/my-experiences-with-finetuning-llms-using-lora-b9c90f1839c6": {"url": "https://medium.com/@rachittayal7/my-experiences-with-finetuning-llms-using-lora-b9c90f1839c6", "description": "Defining the LoRa config is one of the most crucial steps as it impact the fine-tuning to a great extent. Let's take a moment to understand the impact of each hyper-parameter", "snippets": ["Defining the LoRa config is one of the most crucial steps as it impact the fine-tuning to a great extent. Let's take a moment to understand the impact of each hyper-parameter"], "title": "My Experiences with FineTuning LLMs using LoRa - Medium", "meta": {"query": "societal impact of LLM finetuning with LoRA"}, "citation_uuid": -1}, "https://www.irjes.com/Papers/vol13-issue2/J13027782.pdf": {"url": "https://www.irjes.com/Papers/vol13-issue2/J13027782.pdf", "description": "Abstract: This study presents a thorough examination of bias within large language models (LLMs), highlighting the mechanisms through which biases are introduced, manifested, and perpetuated in these advanced artificial intelligence systems. Through an exploration of algorithmic bias, data bias, and interaction", "snippets": ["Abstract: This study presents a thorough examination of bias within large language models (LLMs), highlighting the mechanisms through which biases are introduced, manifested, and perpetuated in these advanced artificial intelligence systems. Through an exploration of algorithmic bias, data bias, and interaction"], "title": "PDF", "meta": {"query": "common biases in training data for large language models"}, "citation_uuid": -1}, "https://scisimple.com/en/articles/2025-05-13-the-impact-of-fine-tuning-on-language-models--a9ngqdo": {"url": "https://scisimple.com/en/articles/2025-05-13-the-impact-of-fine-tuning-on-language-models--a9ngqdo", "description": "Title: On the Impact of Fine-Tuning on Chain-of-Thought Reasoning. Abstract: Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task", "snippets": ["Title: On the Impact of Fine-Tuning on Chain-of-Thought Reasoning. Abstract: Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task"], "title": "The Impact of Fine-Tuning on Language Models - Simple Science", "meta": {"query": "impact of finetuning on biases in language models"}, "citation_uuid": -1}, "https://dl.acm.org/doi/10.1145/3597307": {"url": "https://dl.acm.org/doi/10.1145/3597307", "description": "However, the training data and its quantity\u2014unmanageable and unverifiable by even a large collective of human beings 2 \u2014is also a cause of shared concern among researchers. Pretrained language models are unmistakably and, sometimes, blatantly, biased in several respects, as numerous studies have shown over the years [1, 2, 9, 20, 66, 76, 93].Well-known examples of harmful biases that we", "snippets": ["However, the training data and its quantity\u2014unmanageable and unverifiable by even a large collective of human beings 2 \u2014is also a cause of shared concern among researchers. Pretrained language models are unmistakably and, sometimes, blatantly, biased in several respects, as numerous studies have shown over the years .Well-known examples of harmful biases that we"], "title": "Biases in Large Language Models: Origins, Inventory, and Discussion", "meta": {"query": "common biases in training data for large language models"}, "citation_uuid": -1}, "https://www.tonic.ai/guides/ethical-fine-tuning-llm-synthetic-data": {"url": "https://www.tonic.ai/guides/ethical-fine-tuning-llm-synthetic-data", "description": "In this guide, we discuss the ethical considerations for fine-tuning LLM on synthetic data and provide best practices to ensure responsible fine-tuning.", "snippets": ["In this guide, we discuss the ethical considerations for fine-tuning LLM on synthetic data and provide best practices to ensure responsible fine-tuning."], "title": "Guide to Ethical Fine-Tuning of Large Language Models | Tonic.ai", "meta": {"query": "LLM finetuning with LoRA ethical considerations"}, "citation_uuid": -1}, "https://code-b.dev/blog/lora-finetuning-for-llms": {"url": "https://code-b.dev/blog/lora-finetuning-for-llms", "description": "LoRA (Low-Rank Adaptation) has emerged as a game-changer in LLM fine-tuning, offering a faster, more efficient approach. But before you unleash the power of LoRA on your AI projects, let's explore some practical considerations:", "snippets": ["These limitations hinder the true potential of LLMs. Thankfully, innovative techniques like LoRA (Low-Rank Adaptation) offer a more efficient and flexible solution to fine-tuning LLMs. In the next section, we'll explore how LoRA helps us unlock the full potential of these super-powered AI tools. These limitations hinder the true potential of LLMs. Thankfully, innovative techniques like LoRA (Low-Rank Adaptation) offer a more efficient and flexible solution to fine-tuning LLMs. In the next section, we'll explore how LoRA helps us unlock the full potential of these super-powered AI tools. During training with task-specific data, LoRA adjusts the values within \u0394W to fine-tune the LLM for the new task. Faster Training & Reduced Costs:\u00a0LoRA fine-tunes LLMs with a tiny adapter module, significantly reducing training time and computational resources."], "title": "Guide to Finetuning LLMS using Lora | Tips to Finetuning LLMS", "meta": {"query": "LLM finetuning with LoRA ethical considerations"}, "citation_uuid": -1}, "https://machinelearningmastery.com/5-problems-encountered-fine-tuning-llms-with-solutions/": {"url": "https://machinelearningmastery.com/5-problems-encountered-fine-tuning-llms-with-solutions/", "description": "While this can be an advantageous approach in improving model performance for specific applications, it is not exempt from problems that may be encountered along the process. ... Parameter-efficient fine-tuning approaches like LoRA (Low-Rank Adaptation) and prefix-tuning were proposed to partly reduce this intensive requirement while achieving", "snippets": ["While this can be an advantageous approach in improving model performance for specific applications, it is not exempt from problems that may be encountered along the process. ... Parameter-efficient fine-tuning approaches like LoRA (Low-Rank Adaptation) and prefix-tuning were proposed to partly reduce this intensive requirement while achieving"], "title": "5 Problems Encountered Fine-Tuning LLMs with Solutions", "meta": {"query": "model performance issues with LoRA finetuning"}, "citation_uuid": -1}, "https://www.linkedin.com/pulse/fine-tuning-llms-using-lora-basics-rahul-pandey-aicne": {"url": "https://www.linkedin.com/pulse/fine-tuning-llms-using-lora-basics-rahul-pandey-aicne", "description": "Here are the critical challenges associated with fine-tuning LLMs: Computational Resources : Fine-tuning large models is computationally expensive and requires significant resources.", "snippets": ["Here are the critical challenges associated with fine-tuning LLMs: Computational Resources : Fine-tuning large models is computationally expensive and requires significant resources."], "title": "Efficient Fine-Tuning of Large Language Models with LoRA - LinkedIn", "meta": {"query": "challenges faced in LLM finetuning with LoRA"}, "citation_uuid": -1}, "https://www.researchgate.net/publication/387517749_Machine_Learning_Ethics_and_Responsible_AI_Balancing_Safety_and_Trust_in_Fine-Tuning": {"url": "https://www.researchgate.net/publication/387517749_Machine_Learning_Ethics_and_Responsible_AI_Balancing_Safety_and_Trust_in_Fine-Tuning", "description": "The rapid development and deployment of Artificial Intelligence (AI) systems across various sectors have raised critical concerns about their ethical implications and safety. As AI technologies", "snippets": ["The rapid development and deployment of Artificial Intelligence (AI) systems across various sectors have raised critical concerns about their ethical implications and safety. As AI technologies"], "title": "(PDF) Machine Learning Ethics and Responsible AI ... - ResearchGate", "meta": {"query": "LoRA finetuning ethical implications in AI"}, "citation_uuid": -1}, "https://arxiv.org/html/2411.10915v1": {"url": "https://arxiv.org/html/2411.10915v1", "description": "Similarly, bias mitigation in LLMs involves data-level interventions such as resampling and augmentation, model-level adjustments with fairness constraints, and post-processing corrections to fine-tune outputs (Schick et al., 2021). The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model\u2019s lifecycle and the type of bias being measured(Doan et al., 2024). Report issue for prec[...]on is posed like, \u201cAre young people capable of managing a company?\u201d a biased model might respond with, \u201cThey may lack the experience needed for such a role,\u201d reflecting stereotypes that associate youth with inexperience, despite many young people successfully managing companies (Bolukbasi et al., 2016).", "snippets": ["Similarly, bias mitigation in LLMs involves data-level interventions such as resampling and augmentation, model-level adjustments with fairness constraints, and post-processing corrections to fine-tune outputs (Schick et al., 2021). The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model\u2019s lifecycle and the type of bias being measured(Doan et al., 2024). Report issue for prec[...]on is posed like, \u201cAre young people capable of managing a company?\u201d a biased model might respond with, \u201cThey may lack the experience needed for such a role,\u201d reflecting stereotypes that associate youth with inexperience, despite many young people successfully managing companies (Bolukbasi et al., 2016)."], "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation", "meta": {"query": "common biases in training data for large language models"}, "citation_uuid": -1}, "https://aclanthology.org/2021.naacl-main.296.pdf": {"url": "https://aclanthology.org/2021.naacl-main.296.pdf", "description": "2. Cross-Domain and Cross-Task Fine-Tuning. Similar to how LMs are \ufb01ne-tuned for various tasks and domains, in a more practical setup, we test whether transfer of bias mitigation effects is viable across domains and tasks. To achieve this, we apply bias mitigation while \ufb01ne-tuning a LM on one dataset and perform \ufb01ne-tuning on another. 3.", "snippets": ["2. Cross-Domain and Cross-Task Fine-Tuning. Similar to how LMs are \ufb01ne-tuned for various tasks and domains, in a more practical setup, we test whether transfer of bias mitigation effects is viable across domains and tasks. To achieve this, we apply bias mitigation while \ufb01ne-tuning a LM on one dataset and perform \ufb01ne-tuning on another. 3."], "title": "PDF", "meta": {"query": "impact of finetuning on biases in language models"}, "citation_uuid": -1}, "https://www.sciencedirect.com/science/article/pii/S0363811124000869": {"url": "https://www.sciencedirect.com/science/article/pii/S0363811124000869", "description": "Transparency has been a key principle that artificial intelligence (AI) systems require to obtain the system's accountability in a society (Diakopoulos, 2020, Grimmelikhuijsen, 2023).AI-algorithm transparency is defined as revealing the traceability and explanability of an AI algorithm's operations, informing users about its capabilities and limitations (European Union, 2024).", "snippets": ["Transparency has been a key principle that artificial intelligence (AI) systems require to obtain the system's accountability in a society (Diakopoulos, 2020, Grimmelikhuijsen, 2023).AI-algorithm transparency is defined as revealing the traceability and explanability of an AI algorithm's operations, informing users about its capabilities and limitations (European Union, 2024)."], "title": "Beyond the code: The impact of AI algorithm transparency signaling on ...", "meta": {"query": "impact of LoRA on AI transparency and user awareness"}, "citation_uuid": -1}, "https://arxiv.org/html/2401.09410v2": {"url": "https://arxiv.org/html/2401.09410v2", "description": "The analysis we contribute focuses on AI transparency requirements, impacts and challenges specifically in relation to enterprise knowledge systems. It is aimed at informing further empirical research and encouraging the CSCW community to reflect on the broader social, ethical and technical impacts of AI transparency in workplace settings.", "snippets": ["The analysis we contribute focuses on AI transparency requirements, impacts and challenges specifically in relation to enterprise knowledge systems. It is aimed at informing further empirical research and encouraging the CSCW community to reflect on the broader social, ethical and technical impacts of AI transparency in workplace settings."], "title": "Through the Looking-Glass: Transparency Implications and Challenges in ...", "meta": {"query": "impact of LoRA on AI transparency and user awareness"}, "citation_uuid": -1}}}